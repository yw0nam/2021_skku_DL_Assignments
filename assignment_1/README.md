1. What happens to the gradient if you backpropagate through a long sequence

it cause numerically unstable and gives undue importance to potentially irrelvant past detail

2. What happens if you increase the number of hidden layers in the RNN model

I can't answer it. In many case, increasing the number of hidden layers is good solution for model performance.
But, it has a risk of overfitting or gradient vanishment.
